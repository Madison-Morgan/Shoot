{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification for Rock Paper Scissors Moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Madison Morgan, 2020\n",
    "\n",
    "Here is a notebook that demos two different types of deep learning image classifiers.\n",
    "For Rock Paper Scissors Moves to be used in an app. Here is demonstrated and tested:\n",
    "    A developed CNN model and a MobileVNET model.\n",
    "    \n",
    "Credits: I referenced a few notebooks, tutorials, and data \n",
    "* https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb#scrollTo=DJ8jGFnTLt8t\n",
    "* https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.0 [Importing Packages and Defining Constants](#1.0)\n",
    "* 2.0 [Data Preprocessing](#2.0)\n",
    "    * 2.1 [Data Preprocessing: CNN Model](#2.1)\n",
    "    * 2.2 [Data Preprocessing: MobileNetV2](#2.2)\n",
    "* 3.0 [Transforming the Data](#3.0)\n",
    "* 4.0 [Augmenting the Data](#4.0)\n",
    "* 5.0 [Preparing the Data for Model](#5.0)\n",
    "    * 5.1 [Defining Data Generators](#5.1)\n",
    "    * 5.2 [Shuffling the Data](#5.2)\n",
    "* 6.0 [Defining the Models](#6.0)\n",
    "    * 6.1[Defining the Models: CNN Model](#6.1)\n",
    "    * 6.2[Defining the Models: MobileNetV2](#6.2)\n",
    "* 7.0 [Training the Models](#7.0)\n",
    "    * 7.1[Training the Models: CNN Model](#7.1)\n",
    "    * 7.2[Training the Models: MobileNetV2](#7.2)\n",
    "* 8.0 [Evaluating Accuracy and Loss of Model](#8.0)\n",
    "* 9.0 [Testing the Model](#9.0)\n",
    "* 10.0 [Saving Model as TF.Lite for App](#10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages and Defining Constants <a class=\"anchor\" id=\"1.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some constants\n",
    "DATA_PATH =\"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//downloaded//\"\n",
    "TRAIN_PATH = \"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//data//train//\"\n",
    "TEST_PATH = \"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//data//test//\"\n",
    "VALIDATION_PATH = \"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//data//validation//\"\n",
    "BEST_MODEL_PATH = \"best_model.h5\"\n",
    "INPUT_SHAPE = (128,128,3)\n",
    "TARGET_SIZE = (128,128)\n",
    "BATCH_SIZE = 32\n",
    "CLASS_MODE = 'categorical'\n",
    "TF_DATASET = 'rock_paper_scissors'\n",
    "TRAINING_SIZE = 0.9 #train CNN with 90% of images\n",
    "TESTING_SIZE = 0.05 #test CNN with 5% of images\n",
    "VALIDATION_SIZE = 0.05 #validate CNN with 5% of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing <a class=\"anchor\" id=\"2.0\"></a>\n",
    "Importing the data and seeing what the heck it looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: CNN Model <a class=\"anchor\" id=\"2.1\"></a>\n",
    "Need only run once, hence commented out to avoid accidentally rerunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(os.listdir(DATA_PATH+\"/paper\")))\n",
    "#print(len(os.listdir(DATA_PATH+\"/rock\")))\n",
    "#print(len(os.listdir(DATA_PATH+\"/scissors\")))\n",
    "\n",
    "#def moveFiles(files,src,dest):\n",
    " #   for file in files:\n",
    "  #      full_file_name = os.path.join(src, file)\n",
    "   #     if os.path.isfile(full_file_name):\n",
    "    #        shutil.copy(full_file_name,dest)\n",
    "\n",
    "\n",
    "#def separateData():\n",
    " #   for move in ['paper','rock','scissors']:\n",
    "  #      files = os.listdir(DATA_PATH+move)\n",
    "   #     train_index = math.ceil(len(files)*0.9)\n",
    "    #    test_index = train_index+ math.ceil(len(files)*0.05)\n",
    "     #   validation_index = test_index+ math.floor(len(files)*0.05)\n",
    "    \n",
    "      #  moveFiles(files[:train_index],DATA_PATH+move,TRAIN_PATH+move)\n",
    "       # moveFiles(files[train_index:test_index],DATA_PATH+move,TEST_PATH+move)\n",
    "        #moveFiles(files[test_index:validation_index],DATA_PATH+move,VALIDATION_PATH+move)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: MobileNetV2 <a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_train_raw, dataset_test_raw), dataset_info = tfds.load(\n",
    "    name=TF_DATASET,\n",
    "    data_dir='tmp',\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    split=['train','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw train dataset size:', len(list(dataset_train_raw)),)\n",
    "print('Raw test dataset size:', len(list(dataset_test_raw)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mobilenet v2 possible input sizes are [96, 128, 160, 192, 224].\n",
    "INPUT_IMG_SIZE_REDUCED = 224\n",
    "INPUT_IMG_SHAPE_REDUCED = (\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_SHAPE[2]\n",
    ")\n",
    "INPUT_SIZE= INPUT_IMG_SIZE_REDUCED\n",
    "print(str(dataset_info.features['label'].int2str(0)))\n",
    "print(dataset_info.features['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the Data <a class=\"anchor\" id=\"3.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(image, label):\n",
    "    # Make image color values to be float.\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Make image color values to be in [0..1] range.\n",
    "    image = image / 255.\n",
    "    # Make sure that image has a right size\n",
    "    image = tf.image.resize(image, [INPUT_SIZE, INPUT_SIZE])\n",
    "    return image, label\n",
    "\n",
    "#dataset_train = dataset_train_raw.map(format_example)\n",
    "#dataset_test = dataset_test_raw.map(format_example)\n",
    "print(list(dataset_train.take(1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting the Data <a class=\"anchor\" id=\"4.0\"></a>\n",
    "\n",
    "In this way we avoid overfitting the model and are able to generalize the model to a broader set of examples.\n",
    "\n",
    "Consider when the image is horizontal, if the background is not bright, if the User uses the left hand? To adapt the model to more real life scenarios we can flip, rotate, and adjust the background colors of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image,label):\n",
    "    \n",
    "    def augment_flip(image):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        return image\n",
    "    \n",
    "    def augment_color(image):\n",
    "        image = tf.image.random_hue(image, max_delta=0.08)\n",
    "        image = tf.image.random_saturation(image, lower=0.7, upper=1.3)\n",
    "        image = tf.image.random_brightness(image, 0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.8, upper=1)\n",
    "        image = tf.clip_by_value(image, clip_value_min=0, clip_value_max=1)\n",
    "        return image\n",
    "    \n",
    "    def augment_rotate(image):\n",
    "        # Rotate 0, 90, 180, 270 degrees\n",
    "        return tf.image.rot90(\n",
    "        image,\n",
    "        tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    \n",
    "    def augment_invert(image):\n",
    "        random = tf.random.uniform(shape=[], minval=0, maxval=1)\n",
    "        if random > 0.5:\n",
    "            image = tf.math.multiply(image, -1)\n",
    "            image = tf.math.add(image, 1)\n",
    "        return image\n",
    "    \n",
    "    def augment_zoom(image):\n",
    "        image_width, image_height, image_colors = image.shape\n",
    "        crop_size = (image_width, image_height)\n",
    "        min_zoom, max_zoom = 0.8, 1.0\n",
    "        \n",
    "        # Generate crop settings, ranging from a 1% to 20% crop.\n",
    "        scales = list(np.arange(min_zoom, max_zoom, 0.01))\n",
    "        boxes = np.zeros((len(scales), 4))\n",
    "\n",
    "        for i, scale in enumerate(scales):\n",
    "            x1 = y1 = 0.5 - (0.5 * scale)\n",
    "            x2 = y2 = 0.5 + (0.5 * scale)\n",
    "            boxes[i] = [x1, y1, x2, y2]\n",
    "\n",
    "        def random_crop(img):\n",
    "            # Create different crops for an image\n",
    "            crops = tf.image.crop_and_resize(\n",
    "            [img],\n",
    "            boxes=boxes,\n",
    "            box_indices=np.zeros(len(scales)),\n",
    "            crop_size=crop_size)\n",
    "            # Return a random crop\n",
    "            return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
    "\n",
    "        choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "\n",
    "        # Only apply cropping 50% of the time\n",
    "        return tf.cond(choice < 0.5, lambda: image, lambda: random_crop(image))\n",
    "     \n",
    "        \n",
    "    image = augment_flip(image)\n",
    "    image = augment_color(image)\n",
    "    image = augment_rotate(image)\n",
    "    image = augment_zoom(image)\n",
    "    image = augment_invert(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "train_data_augmented = dataset_train.map(augment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data for Model <a class=\"anchor\" id=\"5.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Generators <a class=\"anchor\" id=\"5.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1. / 255,\n",
    "                            rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "validation_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE\n",
    ")\n",
    "\n",
    "\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=1,\n",
    "    class_mode=CLASS_MODE\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_generator = validation_generator.flow_from_directory(\n",
    "    VALIDATION_PATH,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the Data <a class=\"anchor\" id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 800\n",
    "\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented.shuffle(\n",
    "    buffer_size=NUM_TRAIN_EXAMPLES\n",
    ")\n",
    "\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented.batch(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Prefetch will enable the input pipeline to asynchronously fetch batches while your model is training.\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented_shuffled.prefetch(\n",
    "    buffer_size=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "dataset_test_shuffled = dataset_test.batch(BATCH_SIZE)\n",
    "batches = tfds.as_numpy(dataset_train_augmented_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model <a class=\"anchor\" id=\"6.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model: CNN Model <a class=\"anchor\" id=\"6.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, dataPath, inputShape):\n",
    "        self.datapath = dataPath\n",
    "        self.model = self.build_model(inputShape)\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "        \n",
    "    \n",
    "    def build_model(self, inputShape):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=inputShape, activation='relu'))\n",
    "        #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        #model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        \n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = CNN(DATA_PATH, INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    BEST_MODEL_PATH, \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_callback  = ReduceLROnPlateau(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 3,\n",
    "    factor = 0.5,\n",
    "    min_lr = 0.00001,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint_callback, reduce_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model: MobileNetV2 for Feature Extraction <a class=\"anchor\" id=\"6.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "  input_shape=INPUT_SHAPE,\n",
    "  include_top=False,\n",
    "  weights='imagenet',\n",
    "  pooling='avg'\n",
    ")\n",
    "\n",
    "# Freezing base model, dont want to retrain only want feature extraction!\n",
    "base_model.trainable= False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "base_model,\n",
    "show_shapes=True,\n",
    "show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "\n",
    "# model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=NUM_CLASSES,\n",
    "    activation=tf.keras.activations.softmax,\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(l=0.01)\n",
    "))\n",
    "\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=rmsprop_optimizer,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model <a class=\"anchor\" id=\"7.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model : CNN Model <a class=\"anchor\" id=\"7.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_train = train_generator.n//train_generator.batch_size\n",
    "step_size_val = validation_generator.n//validation_generator.batch_size\n",
    "CNN.history = CNN.model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=25,\n",
    "                   validation_data=validation_generator,\n",
    "                   validation_steps=step_size_val,\n",
    "                   verbose=1,\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model : MobileNetV2 <a class=\"anchor\" id=\"7.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Accuracy and Loss of Model <a class=\"anchor\" id=\"8.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = CNN.history.history['acc']\n",
    "validation_accuracy = CNN.history.history['val_acc']\n",
    "loss = CNN.history.history['loss']\n",
    "validation_loss = CNN.history.history['val_loss']\n",
    "\n",
    "num_epochs = range(len(accuracy))\n",
    "plt.plot(num_epochs, accuracy, 'r', label='Training Accuracy')\n",
    "plt.plot(num_epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(num_epochs, validation_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model <a class=\"anchor\" id=\"9.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN.model.load_weights(BEST_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_test = test_generator.n//test_generator.batch_size\n",
    "testing_model = CNN.model.evaluate_generator(\n",
    "    test_generator,\n",
    "    step_size_test,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = CNN.model.predict_generator(test_generator, test_generator.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "path = \"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//data///testNEWPHOTOS//\"\n",
    "for file in os.listdir(path):\n",
    "    img = Image.open(path+file)#, target_size(60,60))\n",
    "    #print(img.mode)\n",
    "    img.load()\n",
    "    background = Image.new(\"RGB\", img.size, (255,255,255))\n",
    "    background.paste(img,mask=img.split()[3])\n",
    "    img = background\n",
    "    img = img.resize((60,60)) \n",
    "    img = image.img_to_array(img)\n",
    "    #print(img.shape)\n",
    "    imgplot = plt.imshow(img)\n",
    "    img = img.reshape((1,)+img.shape)\n",
    "    #print(img.shape)\n",
    "    \n",
    "    \n",
    "    img_class = CNN.model.predict_classes(img)\n",
    "    print(\"File: \"+str(file)+\", pred: \"+str(img_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model as TF.Lite for App <a class=\"anchor\" id=\"10.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"best_model.h5\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
