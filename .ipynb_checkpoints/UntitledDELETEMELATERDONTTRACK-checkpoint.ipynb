{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_train_raw, dataset_test_raw), dataset_info = tfds.load(\n",
    "    name=TF_DATASET,\n",
    "    data_dir='tmp',\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    split=['train','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_train_raw, dataset_test_raw), dataset_info = tfds.load(\n",
    "    name=TF_DATASET,\n",
    "    data_dir='tmp',\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    split=['train','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mobilenet v2 possible input sizes are [96, 128, 160, 192, 224].\n",
    "# the data is (300,300,3)\n",
    "INPUT_IMG_SIZE_REDUCED = 128\n",
    "INPUT_IMG_SHAPE_REDUCED = (\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_SHAPE[2]\n",
    ")\n",
    "INPUT_SIZE= INPUT_IMG_SIZE_REDUCED\n",
    "print(str(dataset_info.features['label'].int2str(0)))\n",
    "print(dataset_info.features['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important here, label 0 = rock, label 1 = paper, label 2 = scissors\n",
    "label_names = dataset_info.features['label'].int2str\n",
    "print(label_names(0), label_names(1), label_names(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show data\n",
    "(image,label) = dataset_train_raw.take(1)[0]\n",
    "print(image.numpy().shape)\n",
    "plt.imshow(image.numpy())\n",
    "plt.title(\"label: \"+label_name(label.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Data for Model <a class=\"anchor\" id=\"3.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming the Data <a class=\"anchor\" id=\"3.1\"></a>\n",
    "Should be noted that this step is already done when importing the data with ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(image, label):\n",
    "    # Make image color values to be float.\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Make image color values to be in [0..1] range.\n",
    "    image = image / 255.\n",
    "    # Make sure that image has a right size\n",
    "    image = tf.image.resize(image, [INPUT_SIZE, INPUT_SIZE])\n",
    "    return image, label\n",
    "\n",
    "dataset_train = dataset_train_raw.map(format_example)\n",
    "dataset_test = dataset_test_raw.map(format_example)\n",
    "#print(list(dataset_train.take(1))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmenting the Data <a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "In this way we avoid overfitting the model and are able to generalize the model to a broader set of examples.\n",
    "\n",
    "Consider when the image is horizontal, if the background is not bright, if the User uses the left hand? To adapt the model to more real life scenarios we can flip, rotate, and adjust the background colors of the images\n",
    "\n",
    "Should be noted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image,label):\n",
    "    \n",
    "    def augment_flip(image):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        return image\n",
    "    \n",
    "    def augment_color(image):\n",
    "        image = tf.image.random_hue(image, max_delta=0.5)\n",
    "        image = tf.image.random_saturation(image, lower=0.7, upper=3)\n",
    "        image = tf.image.random_brightness(image, 0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.8, upper=1)\n",
    "        image = tf.clip_by_value(image, clip_value_min=0, clip_value_max=1)\n",
    "        return image\n",
    "    \n",
    "    def augment_rotate(image):\n",
    "        # Rotate 0, 90, 180, 270 degrees\n",
    "        return tf.image.rot90(\n",
    "        image,\n",
    "        tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    \n",
    "    def augment_invert(image):\n",
    "        random = tf.random.uniform(shape=[], minval=0, maxval=1)\n",
    "        if random > 0.5:\n",
    "            image = tf.math.multiply(image, -1)\n",
    "            image = tf.math.add(image, 1)\n",
    "        return image\n",
    "    \n",
    "    def augment_zoom(image):\n",
    "        image_width, image_height, image_colors = image.shape\n",
    "        crop_size = (image_width, image_height)\n",
    "        min_zoom, max_zoom = 0.8, 1.0\n",
    "        \n",
    "        # Generate crop settings, ranging from a 1% to 20% crop.\n",
    "        scales = list(np.arange(min_zoom, max_zoom, 0.01))\n",
    "        boxes = np.zeros((len(scales), 4))\n",
    "\n",
    "        for i, scale in enumerate(scales):\n",
    "            x1 = y1 = 0.5 - (0.5 * scale)\n",
    "            x2 = y2 = 0.5 + (0.5 * scale)\n",
    "            boxes[i] = [x1, y1, x2, y2]\n",
    "\n",
    "        def random_crop(img):\n",
    "            # Create different crops for an image\n",
    "            crops = tf.image.crop_and_resize(\n",
    "            [img],\n",
    "            boxes=boxes,\n",
    "            box_indices=np.zeros(len(scales)),\n",
    "            crop_size=crop_size)\n",
    "            # Return a random crop\n",
    "            return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
    "\n",
    "        choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "\n",
    "        # Only apply cropping 50% of the time\n",
    "        return tf.cond(choice < 0.5, lambda: image, lambda: random_crop(image))\n",
    "     \n",
    "    def augment_grayscale(img):\n",
    "        return tf.image.rgb_to_grayscale(img)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    image = augment_flip(image)\n",
    "    image = augment_color(image)\n",
    "    image = augment_rotate(image)\n",
    "    image = augment_zoom(image)\n",
    "    image = augment_invert(image)\n",
    "    image = augment_grayscale(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "train_data_augmented = dataset_train.map(augment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffling the Data <a class=\"anchor\" id=\"3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 800\n",
    "\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented.shuffle(\n",
    "    buffer_size=NUM_TRAIN_EXAMPLES\n",
    ")\n",
    "\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented.batch(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Prefetch will enable the input pipeline to asynchronously fetch batches while your model is training.\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented_shuffled.prefetch(\n",
    "    buffer_size=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "dataset_test_shuffled = dataset_test.batch(BATCH_SIZE)\n",
    "batches = tfds.as_numpy(dataset_train_augmented_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging the batches using conversion to Numpy arrays.\n",
    "batches = tfds.as_numpy(dataset_train_augmented_shuffled)\n",
    "for batch in batches:\n",
    "    image_batch, label_batch = batch\n",
    "    print('Label batch shape:', label_batch.shape, '\\n')\n",
    "    print('Image batch shape:', image_batch.shape, '\\n')\n",
    "    print('Label batch:', label_batch, '\\n')\n",
    "    \n",
    "    for batch_item_index in range(len(image_batch)):\n",
    "        print('First batch image:', image_batch[batch_item_index], '\\n')\n",
    "        plt.imshow(image_batch[batch_item_index])\n",
    "        plt.show()\n",
    "        # Break to shorten the output.\n",
    "        break\n",
    "    # Break to shorten the output.\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
