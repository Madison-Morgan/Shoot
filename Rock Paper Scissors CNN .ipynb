{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification for Rock Paper Scissors Moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a notebook that demos two different types of deep learning image classifiers.\n",
    "For Rock Paper Scissors Moves to be used in an app. Here is demonstrated and tested:\n",
    "    A developed CNN model and a MobileVNET model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing some Packages and Defining Some Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some constants\n",
    "DATA_PATH =\"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//downloaded//\"\n",
    "TRAIN_PATH = \"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//data//train//\"\n",
    "TEST_PATH = \"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//data//test//\"\n",
    "VALIDATION_PATH = \"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//data//validation//\"\n",
    "BEST_MODEL_PATH = \"best_model.h5\"\n",
    "INPUT_SHAPE = (200,200,3)\n",
    "TARGET_SIZE = (200,200)\n",
    "BATCH_SIZE = 32\n",
    "CLASS_MODE = 'categorical'\n",
    "TF_DATASET = 'rock_paper_scissors'\n",
    "TRAINING_SIZE = 0.9 #train CNN with 90% of images\n",
    "TESTING_SIZE = 0.05 #test CNN with 5% of images\n",
    "VALIDATION_SIZE = 0.05 #validate CNN with 5% of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Need only run once, hence commented out to avoid accidentally rerunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n",
      "726\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "#print(len(os.listdir(DATA_PATH+\"/paper\")))\n",
    "#print(len(os.listdir(DATA_PATH+\"/rock\")))\n",
    "#print(len(os.listdir(DATA_PATH+\"/scissors\")))\n",
    "\n",
    "#def moveFiles(files,src,dest):\n",
    " #   for file in files:\n",
    "  #      full_file_name = os.path.join(src, file)\n",
    "   #     if os.path.isfile(full_file_name):\n",
    "    #        shutil.copy(full_file_name,dest)\n",
    "\n",
    "\n",
    "#def separateData():\n",
    " #   for move in ['paper','rock','scissors']:\n",
    "  #      files = os.listdir(DATA_PATH+move)\n",
    "   #     train_index = math.ceil(len(files)*0.9)\n",
    "    #    test_index = train_index+ math.ceil(len(files)*0.05)\n",
    "     #   validation_index = test_index+ math.floor(len(files)*0.05)\n",
    "    \n",
    "      #  moveFiles(files[:train_index],DATA_PATH+move,TRAIN_PATH+move)\n",
    "       # moveFiles(files[train_index:test_index],DATA_PATH+move,TEST_PATH+move)\n",
    "        #moveFiles(files[test_index:validation_index],DATA_PATH+move,VALIDATION_PATH+move)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset rock_paper_scissors/3.0.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to tmp\\rock_paper_scissors\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797cfe2466ba4b7f8b581839e11ccc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1db58228e549e69f7676431700af23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to tmp\\rock_paper_scissors\\3.0.0.incompleteK3BRIX\\rock_paper_scissors-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436490476ee94f53ad9ca97c6c9aaec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to tmp\\rock_paper_scissors\\3.0.0.incompleteK3BRIX\\rock_paper_scissors-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8615de866f2543a98932049efed71450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=372.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset rock_paper_scissors downloaded and prepared to tmp\\rock_paper_scissors\\3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "(dataset_train_raw, dataset_test_raw), dataset_info = tfds.load(\n",
    "    name=TF_DATASET,\n",
    "    data_dir='tmp',\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    split=['train','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw train dataset size: 2520\n",
      "Raw test dataset size: 372 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Raw train dataset size:', len(list(dataset_train_raw)),)\n",
    "print('Raw test dataset size:', len(list(dataset_test_raw)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n",
      "(300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "# For Mobilenet v2 possible input sizes are [96, 128, 160, 192, 224].\n",
    "INPUT_IMG_SIZE_REDUCED = 224\n",
    "INPUT_IMG_SHAPE_REDUCED = (\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_SHAPE[2]\n",
    ")\n",
    "INPUT_SIZE= INPUT_IMG_SIZE_REDUCED\n",
    "print(str(dataset_info.features['label'].int2str(0)))\n",
    "print(dataset_info.features['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=31880, shape=(224, 224, 3), dtype=float32, numpy=\n",
      "array([[[0.995526  , 0.995526  , 0.995526  ],\n",
      "        [0.9941408 , 0.9941408 , 0.9941408 ],\n",
      "        [0.99597746, 0.99597746, 0.99597746],\n",
      "        ...,\n",
      "        [0.9869748 , 0.9869748 , 0.9869748 ],\n",
      "        [0.98237604, 0.98237604, 0.98237604],\n",
      "        [0.97995263, 0.97995263, 0.97995263]],\n",
      "\n",
      "       [[0.99607843, 0.99607843, 0.99607843],\n",
      "        [0.99509835, 0.99509835, 0.99509835],\n",
      "        [0.99578613, 0.99578613, 0.99578613],\n",
      "        ...,\n",
      "        [0.98232853, 0.98232853, 0.98232853],\n",
      "        [0.98235357, 0.98235357, 0.98235357],\n",
      "        [0.9824342 , 0.9824342 , 0.9824342 ]],\n",
      "\n",
      "       [[0.99607843, 0.99607843, 0.99607843],\n",
      "        [0.99438554, 0.99438554, 0.99438554],\n",
      "        [0.9955736 , 0.9955736 , 0.9955736 ],\n",
      "        ...,\n",
      "        [0.982799  , 0.982799  , 0.982799  ],\n",
      "        [0.97900224, 0.97900224, 0.97900224],\n",
      "        [0.98414266, 0.98414266, 0.98414266]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.9886986 , 0.9886986 , 0.9886986 ],\n",
      "        [0.98788357, 0.98788357, 0.98788357],\n",
      "        [0.98773044, 0.98773044, 0.98773044],\n",
      "        ...,\n",
      "        [0.97477514, 0.97477514, 0.97477514],\n",
      "        [0.9725384 , 0.9725384 , 0.9725384 ],\n",
      "        [0.96988803, 0.96988803, 0.96988803]],\n",
      "\n",
      "       [[0.98982257, 0.98982257, 0.98982257],\n",
      "        [0.9872209 , 0.9872209 , 0.9872209 ],\n",
      "        [0.98630947, 0.98630947, 0.98630947],\n",
      "        ...,\n",
      "        [0.9689198 , 0.9689198 , 0.9689198 ],\n",
      "        [0.97251344, 0.97251344, 0.97251344],\n",
      "        [0.9728876 , 0.9728876 , 0.9728876 ]],\n",
      "\n",
      "       [[0.98945296, 0.98945296, 0.98945296],\n",
      "        [0.9898225 , 0.9898225 , 0.9898225 ],\n",
      "        [0.98757   , 0.98757   , 0.98757   ],\n",
      "        ...,\n",
      "        [0.9692227 , 0.9692227 , 0.9692227 ],\n",
      "        [0.9709499 , 0.9709499 , 0.9709499 ],\n",
      "        [0.9774043 , 0.9774043 , 0.9774043 ]]], dtype=float32)>, <tf.Tensor: id=31881, shape=(), dtype=int64, numpy=2>)\n"
     ]
    }
   ],
   "source": [
    "def format_example(image, label):\n",
    "    # Make image color values to be float.\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Make image color values to be in [0..1] range.\n",
    "    image = image / 255.\n",
    "    # Make sure that image has a right size\n",
    "    image = tf.image.resize(image, [INPUT_SIZE, INPUT_SIZE])\n",
    "    return image, label\n",
    "\n",
    "#dataset_train = dataset_train_raw.map(format_example)\n",
    "#dataset_test = dataset_test_raw.map(format_example)\n",
    "print(list(dataset_train.take(1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting the Data\n",
    "\n",
    "In this way we avoid overfitting the model and are able to generalize the model to a broader set of examples.\n",
    "\n",
    "Consider when the image is horizontal, if the background is not bright, if the User uses the left hand? To adapt the model to more real life scenarios we can flip, rotate, and adjust the background colors of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image,label):\n",
    "    \n",
    "    def augment_flip(image):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        return image\n",
    "    \n",
    "    def augment_color(image):\n",
    "        image = tf.image.random_hue(image, max_delta=0.08)\n",
    "        image = tf.image.random_saturation(image, lower=0.7, upper=1.3)\n",
    "        image = tf.image.random_brightness(image, 0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.8, upper=1)\n",
    "        image = tf.clip_by_value(image, clip_value_min=0, clip_value_max=1)\n",
    "        return image\n",
    "    \n",
    "    def augment_rotate(image):\n",
    "        # Rotate 0, 90, 180, 270 degrees\n",
    "        return tf.image.rot90(\n",
    "        image,\n",
    "        tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    \n",
    "    def augment_invert(image):\n",
    "        random = tf.random.uniform(shape=[], minval=0, maxval=1)\n",
    "        if random > 0.5:\n",
    "            image = tf.math.multiply(image, -1)\n",
    "            image = tf.math.add(image, 1)\n",
    "        return image\n",
    "    \n",
    "    def augment_zoom(image):\n",
    "        image_width, image_height, image_colors = image.shape\n",
    "        crop_size = (image_width, image_height)\n",
    "        min_zoom, max_zoom = 0.8, 1.0\n",
    "        \n",
    "        # Generate crop settings, ranging from a 1% to 20% crop.\n",
    "        scales = list(np.arange(min_zoom, max_zoom, 0.01))\n",
    "        boxes = np.zeros((len(scales), 4))\n",
    "\n",
    "        for i, scale in enumerate(scales):\n",
    "            x1 = y1 = 0.5 - (0.5 * scale)\n",
    "            x2 = y2 = 0.5 + (0.5 * scale)\n",
    "            boxes[i] = [x1, y1, x2, y2]\n",
    "\n",
    "        def random_crop(img):\n",
    "            # Create different crops for an image\n",
    "            crops = tf.image.crop_and_resize(\n",
    "            [img],\n",
    "            boxes=boxes,\n",
    "            box_indices=np.zeros(len(scales)),\n",
    "            crop_size=crop_size)\n",
    "            # Return a random crop\n",
    "            return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
    "\n",
    "        choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "\n",
    "        # Only apply cropping 50% of the time\n",
    "        return tf.cond(choice < 0.5, lambda: image, lambda: random_crop(image))\n",
    "     \n",
    "        \n",
    "    image = augment_flip(image)\n",
    "    image = augment_color(image)\n",
    "    image = augment_rotate(image)\n",
    "    image = augment_zoom(image)\n",
    "    image = augment_invert(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "train_data_augmented = dataset_train.map(augment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1970 images belonging to 3 classes.\n",
      "Found 111 images belonging to 3 classes.\n",
      "Found 107 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1. / 255,\n",
    "                            rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "validation_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE\n",
    ")\n",
    "\n",
    "\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=1,\n",
    "    class_mode=CLASS_MODE\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_generator = validation_generator.flow_from_directory(\n",
    "    VALIDATION_PATH,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built and Developed CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, dataPath, inputShape):\n",
    "        self.datapath = dataPath\n",
    "        self.model = self.build_model(inputShape)\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "        \n",
    "    \n",
    "    def build_model(self, inputShape):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=inputShape, activation='relu'))\n",
    "        #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        #model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        \n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 58, 58, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               4718848   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 4,979,779\n",
      "Trainable params: 4,979,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN = CNN(DATA_PATH, INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    BEST_MODEL_PATH, \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_callback  = ReduceLROnPlateau(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 3,\n",
    "    factor = 0.5,\n",
    "    min_lr = 0.00001,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint_callback, reduce_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.0840 - accuracy: 0.3851\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.41667, saving model to best_model.h5\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 1.0841 - accuracy: 0.3854 - val_loss: 1.0794 - val_accuracy: 0.4167\n",
      "Epoch 2/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 1.0366 - accuracy: 0.4659\n",
      "Epoch 00002: val_accuracy improved from 0.41667 to 0.50000, saving model to best_model.h5\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 1.0369 - accuracy: 0.4665 - val_loss: 1.0147 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.8107 - accuracy: 0.6369\n",
      "Epoch 00003: val_accuracy improved from 0.50000 to 0.67708, saving model to best_model.h5\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 0.8090 - accuracy: 0.6388 - val_loss: 0.6528 - val_accuracy: 0.6771\n",
      "Epoch 4/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.5500 - accuracy: 0.7880\n",
      "Epoch 00004: val_accuracy improved from 0.67708 to 0.75000, saving model to best_model.h5\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.5526 - accuracy: 0.7864 - val_loss: 0.5194 - val_accuracy: 0.7500\n",
      "Epoch 5/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.4901 - accuracy: 0.8048\n",
      "Epoch 00005: val_accuracy improved from 0.75000 to 0.83333, saving model to best_model.h5\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.4899 - accuracy: 0.8039 - val_loss: 0.4391 - val_accuracy: 0.8333\n",
      "Epoch 6/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8631\n",
      "Epoch 00006: val_accuracy improved from 0.83333 to 0.93750, saving model to best_model.h5\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 0.3562 - accuracy: 0.8648 - val_loss: 0.2928 - val_accuracy: 0.9375\n",
      "Epoch 7/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.9150\n",
      "Epoch 00007: val_accuracy did not improve from 0.93750\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.2427 - accuracy: 0.9154 - val_loss: 0.2337 - val_accuracy: 0.8958\n",
      "Epoch 8/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.2112 - accuracy: 0.9307\n",
      "Epoch 00008: val_accuracy improved from 0.93750 to 0.95833, saving model to best_model.h5\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.2112 - accuracy: 0.9303 - val_loss: 0.1325 - val_accuracy: 0.9583\n",
      "Epoch 9/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.2019 - accuracy: 0.9292\n",
      "Epoch 00009: val_accuracy did not improve from 0.95833\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.1989 - accuracy: 0.9303 - val_loss: 0.0894 - val_accuracy: 0.9583\n",
      "Epoch 10/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.1548 - accuracy: 0.9507\n",
      "Epoch 00010: val_accuracy improved from 0.95833 to 0.97917, saving model to best_model.h5\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.1536 - accuracy: 0.9515 - val_loss: 0.0749 - val_accuracy: 0.9792\n",
      "Epoch 11/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9481\n",
      "Epoch 00011: val_accuracy did not improve from 0.97917\n",
      "61/61 [==============================] - 11s 175ms/step - loss: 0.1459 - accuracy: 0.9484 - val_loss: 0.0772 - val_accuracy: 0.9583\n",
      "Epoch 12/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.9559\n",
      "Epoch 00012: val_accuracy did not improve from 0.97917\n",
      "61/61 [==============================] - 10s 171ms/step - loss: 0.1353 - accuracy: 0.9551 - val_loss: 0.1156 - val_accuracy: 0.9479\n",
      "Epoch 13/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9573\n",
      "Epoch 00013: val_accuracy improved from 0.97917 to 0.98958, saving model to best_model.h5\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 0.1249 - accuracy: 0.9580 - val_loss: 0.0625 - val_accuracy: 0.9896\n",
      "Epoch 14/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.9530\n",
      "Epoch 00014: val_accuracy did not improve from 0.98958\n",
      "61/61 [==============================] - 12s 193ms/step - loss: 0.1450 - accuracy: 0.9527 - val_loss: 0.0785 - val_accuracy: 0.9792\n",
      "Epoch 15/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9627 ETA: 0s - loss: 0.1188 - accuracy: \n",
      "Epoch 00015: val_accuracy did not improve from 0.98958\n",
      "61/61 [==============================] - 11s 187ms/step - loss: 0.1187 - accuracy: 0.9628 - val_loss: 0.0401 - val_accuracy: 0.9896\n",
      "Epoch 16/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9717\n",
      "Epoch 00016: val_accuracy did not improve from 0.98958\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "61/61 [==============================] - 11s 186ms/step - loss: 0.0891 - accuracy: 0.9706 - val_loss: 0.0511 - val_accuracy: 0.9896\n",
      "Epoch 17/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9740\n",
      "Epoch 00017: val_accuracy did not improve from 0.98958\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 0.0877 - accuracy: 0.9744 - val_loss: 0.0434 - val_accuracy: 0.9896\n",
      "Epoch 18/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9795\n",
      "Epoch 00018: val_accuracy improved from 0.98958 to 1.00000, saving model to best_model.h5\n",
      "61/61 [==============================] - 11s 172ms/step - loss: 0.0611 - accuracy: 0.9799 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0507 - accuracy: 0.9852\n",
      "Epoch 00019: val_accuracy did not improve from 1.00000\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.0503 - accuracy: 0.9854 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9833\n",
      "Epoch 00020: val_accuracy did not improve from 1.00000\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.0512 - accuracy: 0.9836 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9799\n",
      "Epoch 00021: val_accuracy did not improve from 1.00000\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.0606 - accuracy: 0.9802 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9869\n",
      "Epoch 00022: val_accuracy did not improve from 1.00000\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.0364 - accuracy: 0.9871 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9896\n",
      "Epoch 00023: val_accuracy did not improve from 1.00000\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.0349 - accuracy: 0.9898 - val_loss: 0.0296 - val_accuracy: 0.9896\n",
      "Epoch 24/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0468 - accuracy: 0.9847\n",
      "Epoch 00024: val_accuracy did not improve from 1.00000\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.0468 - accuracy: 0.9844 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0324 - accuracy: 0.9874\n",
      "Epoch 00025: val_accuracy did not improve from 1.00000\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.0326 - accuracy: 0.9871 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9916\n",
      "Epoch 00026: val_accuracy did not improve from 1.00000\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.0261 - accuracy: 0.9917 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0360 - accuracy: 0.9859\n",
      "Epoch 00027: val_accuracy did not improve from 1.00000\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.0356 - accuracy: 0.9862 - val_loss: 0.0190 - val_accuracy: 0.9896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9873\n",
      "Epoch 00028: val_accuracy did not improve from 1.00000\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.0380 - accuracy: 0.9875 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9932\n",
      "Epoch 00029: val_accuracy did not improve from 1.00000\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.0234 - accuracy: 0.9933 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0386 - accuracy: 0.9880\n",
      "Epoch 00030: val_accuracy did not improve from 1.00000\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.0381 - accuracy: 0.9882 - val_loss: 0.0019 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "step_size_train = train_generator.n//train_generator.batch_size\n",
    "step_size_val = validation_generator.n//validation_generator.batch_size\n",
    "CNN.history = CNN.model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=25,\n",
    "                   validation_data=validation_generator,\n",
    "                   validation_steps=step_size_val,\n",
    "                   verbose=1,\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Pre-Developed MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Accuracy and Loss for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = CNN.history.history['acc']\n",
    "validation_accuracy = CNN.history.history['val_acc']\n",
    "loss = CNN.history.history['loss']\n",
    "validation_loss = CNN.history.history['val_loss']\n",
    "\n",
    "num_epochs = range(len(accuracy))\n",
    "plt.plot(num_epochs, accuracy, 'r', label='Training Accuracy')\n",
    "plt.plot(num_epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(num_epochs, validation_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN.model.load_weights(BEST_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 118ms/step - loss: 0.0265 - accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "step_size_test = test_generator.n//test_generator.batch_size\n",
    "testing_model = CNN.model.evaluate_generator(\n",
    "    test_generator,\n",
    "    step_size_test,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = CNN.model.predict_generator(test_generator, test_generator.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.52282450e-04 9.92833257e-01 6.31444622e-03]\n",
      " [3.38490150e-04 1.04784488e-06 9.99660492e-01]\n",
      " [2.46258657e-02 9.74705279e-01 6.68860681e-04]\n",
      " [2.97104862e-06 9.99468267e-01 5.28828532e-04]\n",
      " [3.43536056e-04 9.99272406e-01 3.84104060e-04]\n",
      " [9.98943269e-01 3.49398033e-04 7.07369763e-04]\n",
      " [8.06592579e-04 1.56973942e-06 9.99191821e-01]\n",
      " [9.99999881e-01 1.34374188e-14 6.56784920e-08]\n",
      " [2.27364013e-03 2.15967031e-08 9.97726381e-01]\n",
      " [1.36999530e-03 9.95647371e-01 2.98259384e-03]\n",
      " [4.88483999e-03 8.80944754e-07 9.95114207e-01]\n",
      " [6.29544229e-05 9.99858737e-01 7.83045689e-05]\n",
      " [1.14855054e-03 1.31704310e-05 9.98838246e-01]\n",
      " [1.75255560e-03 7.28056375e-06 9.98240113e-01]\n",
      " [3.13880046e-05 9.99814570e-01 1.54075169e-04]\n",
      " [1.55645991e-02 6.76397467e-04 9.83758926e-01]\n",
      " [4.30244021e-04 9.99487281e-01 8.24578965e-05]\n",
      " [3.12988907e-02 9.00442600e-01 6.82585016e-02]\n",
      " [9.32812877e-03 6.32764946e-04 9.90039051e-01]\n",
      " [9.92160499e-01 5.42197376e-03 2.41755298e-03]\n",
      " [5.78590715e-03 9.77761865e-01 1.64521988e-02]\n",
      " [6.53335452e-01 3.44767809e-01 1.89672166e-03]\n",
      " [9.98921514e-01 3.54518736e-04 7.23951729e-04]\n",
      " [9.99693871e-01 9.74354293e-07 3.05150141e-04]\n",
      " [9.99658704e-01 2.98726954e-05 3.11410986e-04]\n",
      " [8.84845911e-04 4.23235446e-03 9.94882822e-01]\n",
      " [9.99993205e-01 5.82986246e-08 6.83168537e-06]\n",
      " [9.98469293e-01 1.54917357e-10 1.53072248e-03]\n",
      " [1.57400282e-04 6.49380183e-07 9.99841928e-01]\n",
      " [1.39325610e-04 2.24523276e-07 9.99860406e-01]\n",
      " [2.42269471e-05 5.97412509e-07 9.99975204e-01]\n",
      " [9.99994755e-01 1.56175332e-07 5.15640704e-06]\n",
      " [2.27260156e-04 9.99698400e-01 7.44132194e-05]\n",
      " [2.58442294e-02 9.68419790e-01 5.73599525e-03]\n",
      " [6.28494238e-03 9.93694365e-01 2.07165976e-05]\n",
      " [9.99782264e-01 5.82894756e-07 2.17205219e-04]\n",
      " [1.02600209e-01 8.89815271e-01 7.58455321e-03]\n",
      " [9.98072147e-01 5.88291092e-04 1.33959448e-03]\n",
      " [9.45289561e-04 9.98894870e-01 1.59972347e-04]\n",
      " [7.43515696e-03 9.79614556e-01 1.29501941e-02]\n",
      " [9.99994516e-01 1.51430797e-07 5.36042262e-06]\n",
      " [1.51739514e-03 9.98441279e-01 4.13102389e-05]\n",
      " [9.99996543e-01 1.63514457e-07 3.35092795e-06]\n",
      " [9.99863744e-01 1.84301609e-06 1.34471833e-04]\n",
      " [3.30422510e-04 5.90201853e-06 9.99663711e-01]\n",
      " [1.47649518e-03 9.90421176e-01 8.10231268e-03]\n",
      " [2.90565845e-03 1.09663240e-07 9.97094274e-01]\n",
      " [1.19977130e-05 5.30381101e-07 9.99987483e-01]\n",
      " [9.70963180e-01 7.30954926e-05 2.89636850e-02]\n",
      " [5.75879880e-04 9.97772634e-01 1.65142131e-03]\n",
      " [6.41408114e-05 8.53852271e-06 9.99927282e-01]\n",
      " [9.99606550e-01 5.07949169e-11 3.93523456e-04]\n",
      " [9.99608934e-01 5.21950242e-05 3.38917627e-04]\n",
      " [1.57557952e-05 6.96137679e-12 9.99984264e-01]\n",
      " [3.63450759e-04 2.55497231e-04 9.99381065e-01]\n",
      " [9.99921083e-01 1.91716572e-05 5.96795981e-05]\n",
      " [6.60828082e-04 9.99247313e-01 9.17856596e-05]\n",
      " [2.32169428e-03 1.49505376e-03 9.96183217e-01]\n",
      " [6.35422347e-03 9.53283072e-01 4.03627604e-02]\n",
      " [9.32251941e-03 1.12073258e-06 9.90676403e-01]\n",
      " [1.20208249e-03 4.07452880e-08 9.98797894e-01]\n",
      " [6.26647903e-04 9.98470247e-01 9.03030217e-04]\n",
      " [4.82113985e-03 9.93483365e-01 1.69542176e-03]\n",
      " [1.62070082e-03 5.64733000e-07 9.98378754e-01]\n",
      " [1.10628561e-03 9.98708367e-01 1.85335652e-04]\n",
      " [4.62298468e-03 1.06917187e-05 9.95366335e-01]\n",
      " [9.93783057e-01 3.45634110e-03 2.76060821e-03]\n",
      " [8.54340851e-01 1.10077277e-01 3.55817862e-02]\n",
      " [2.44646333e-03 9.89774704e-01 7.77880894e-03]\n",
      " [7.30363885e-03 1.32876266e-05 9.92683113e-01]\n",
      " [9.95645821e-01 1.55819929e-03 2.79600685e-03]\n",
      " [3.66563573e-02 1.29444948e-06 9.63342369e-01]\n",
      " [9.98394310e-01 3.42472980e-04 1.26330904e-03]\n",
      " [1.45186309e-03 9.98346925e-01 2.01241812e-04]\n",
      " [5.66608433e-05 1.54003545e-04 9.99789298e-01]\n",
      " [1.31163368e-04 9.99549806e-01 3.19025421e-04]\n",
      " [9.99972701e-01 2.34496365e-05 3.75504123e-06]\n",
      " [2.01675488e-04 9.75284696e-01 2.45136432e-02]\n",
      " [4.13699570e-04 2.48299625e-06 9.99583781e-01]\n",
      " [9.99847293e-01 2.22122537e-12 1.52702080e-04]\n",
      " [9.96740401e-01 7.52278708e-12 3.25954938e-03]\n",
      " [5.48252836e-04 1.80524324e-07 9.99451578e-01]\n",
      " [4.30723140e-03 9.95574474e-01 1.18273274e-04]\n",
      " [8.78494058e-04 9.98825848e-01 2.95604637e-04]\n",
      " [9.99996066e-01 1.41437919e-08 3.96498899e-06]\n",
      " [1.30727352e-03 9.93292451e-01 5.40030049e-03]\n",
      " [1.20128578e-04 1.84450357e-06 9.99878049e-01]\n",
      " [5.06173633e-03 5.16785476e-05 9.94886577e-01]\n",
      " [9.99953270e-01 6.25507346e-06 4.05376486e-05]\n",
      " [9.99609411e-01 4.14344468e-05 3.49107286e-04]\n",
      " [3.67332309e-01 6.03820443e-01 2.88472790e-02]\n",
      " [4.13469225e-03 9.12765827e-05 9.95774090e-01]\n",
      " [9.99775231e-01 5.70843675e-08 2.24763688e-04]\n",
      " [9.99988914e-01 1.14307203e-10 1.11371573e-05]\n",
      " [3.72424419e-03 9.96165395e-01 1.10423498e-04]\n",
      " [8.85511562e-02 9.02045310e-01 9.40355659e-03]\n",
      " [9.97795463e-01 1.37279423e-08 2.20456626e-03]\n",
      " [3.10929082e-02 5.85697255e-11 9.68907118e-01]\n",
      " [3.76106764e-04 3.50352457e-06 9.99620318e-01]\n",
      " [1.00000000e+00 4.48811043e-20 1.97148093e-08]\n",
      " [4.29737411e-04 6.70225681e-11 9.99570310e-01]\n",
      " [3.16802971e-03 1.54805311e-04 9.96677160e-01]\n",
      " [1.27743697e-02 9.68509197e-01 1.87164824e-02]\n",
      " [3.16803867e-04 9.98415709e-01 1.26743235e-03]\n",
      " [3.88986827e-03 9.84406114e-01 1.17040221e-02]\n",
      " [1.85985482e-05 5.14676728e-08 9.99981403e-01]\n",
      " [2.50712037e-03 8.02304494e-05 9.97412622e-01]\n",
      " [2.68094445e-04 7.09421583e-05 9.99660969e-01]\n",
      " [9.99312758e-01 2.58152810e-04 4.29087842e-04]\n",
      " [1.48400199e-03 9.97821927e-01 6.94123388e-04]\n",
      " [4.62077405e-05 1.13795096e-09 9.99953747e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: paper1.png, pred: [0]\n",
      "File: paper2.png, pred: [0]\n",
      "File: paper3.png, pred: [0]\n",
      "File: rock1.png, pred: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: rock2.png, pred: [0]\n",
      "File: scissors1.png, pred: [0]\n",
      "File: scissors2.png, pred: [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "path = \"C://Users//haide//Desktop//MadyProjectz//RPS//Shoot//data///testNEWPHOTOS//\"\n",
    "for file in os.listdir(path):\n",
    "    img = Image.open(path+file)#, target_size(60,60))\n",
    "    #print(img.mode)\n",
    "    img.load()\n",
    "    background = Image.new(\"RGB\", img.size, (255,255,255))\n",
    "    background.paste(img,mask=img.split()[3])\n",
    "    img = background\n",
    "    img = img.resize((60,60)) \n",
    "    img = image.img_to_array(img)\n",
    "    #print(img.shape)\n",
    "    imgplot = plt.imshow(img)\n",
    "    img = img.reshape((1,)+img.shape)\n",
    "    #print(img.shape)\n",
    "    \n",
    "    \n",
    "    img_class = CNN.model.predict_classes(img)\n",
    "    print(\"File: \"+str(file)+\", pred: \"+str(img_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print(img_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model as TF.Lite for App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"best_model.h5\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
